{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Package load]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print('pytorch version: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.2.2\n",
      "GPU 사용 가능 여부: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이걸 해줘야 matplotlib 시행 시 에러가 안 남\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [For Colab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "folder = \"인공지능 공부\" # 상위 directory\n",
    "project_dir = \"[Kaggle] Retinal OCT Images (optical coherence tomography)제목없는 폴더\" # 폴더 이름\n",
    "base_path = Path(\"/content/drive/MyDrive/\")\n",
    "project_path = base_path / folder / project_dir\n",
    "\n",
    "os.chdir(project_path)\n",
    "for x in list(project_path.glob(\"*\")):\n",
    "    if x.is_dir():\n",
    "        dir_name = str(x.relative_to(project_path))\n",
    "        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
    "print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Setting Hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dataset & DataLoader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/OCT'  # 압축 해제된 데이터셋의 디렉토리 경로\n",
    "\n",
    "# For Kaggle notebook\n",
    "#data_dir = '/kaggle/input/kermany2018/OCT2017 /'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, transform=None):\n",
    "        self.all_data = sorted(glob.glob(os.path.join(data_dir, mode,'*', '*')))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.all_data[index]\n",
    "        img = Image.open(data_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        name = os.path.basename(data_path)\n",
    "        if name.startswith('NORMAL'):\n",
    "            label = 0\n",
    "        elif name.startswith('CNV'):\n",
    "            label = 1\n",
    "        elif name.startswith('DME'):\n",
    "            label = 2\n",
    "        elif name.startswith('DRUSEN'):\n",
    "            label = 3\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.all_data)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.1881,0.1850)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([256]),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.1881,0.1850)\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = OCTDataset(data_dir=data_dir, mode='train', transform=data_transforms['train'])\n",
    "val_data = OCTDataset(data_dir=data_dir, mode='val', transform=data_transforms['val'])\n",
    "test_data = OCTDataset(data_dir=data_dir, mode='test', transform=data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285dbc1460e4182892dc4b37e358378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean: tensor([0.1881]), train std: tensor([0.1850])\n"
     ]
    }
   ],
   "source": [
    "# Calculating mean and std of training data set\n",
    "full_loader = DataLoader(train_data, shuffle=False) #num_workers=os.cpu_count())\n",
    "mean = torch.zeros(1)\n",
    "std = torch.zeros(1)\n",
    "for inputs, _ in tqdm(full_loader):\n",
    "    mean += inputs.mean()\n",
    "    std += inputs.std()\n",
    "mean /= len(train_data)\n",
    "std /= len(train_data)\n",
    "print(f\"train mean: {mean}, train std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = 0.1881\n",
    "train_std = 0.1850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model: VGG16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = [32,32,'M', 64,64,128,128,128,'M',256,256,256,512,512,512,'M'] #13 + 3 =vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        #self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 28 * 28, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = self.avgpool(x)        VGG16 for original ver.\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # cfg = [32,32,'M', 64,64,128,128,128,'M',256,256,256,512,512,512,'M']\n",
    "    def make_layers(cfg, batch_norm=False):\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        \n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "                        \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG(VGG.make_layers(cfg),4,True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=401408, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Saving checkpoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, optimizer, epoch_loss, val_accuracy, saved_dir):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    check_point = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': epoch_loss\n",
    "    }\n",
    "    val_accuracy = round(val_accuracy,1)\n",
    "    file_name = str(f\"ckpoint_model_{epoch+1}_{val_accuracy}%.pt\")\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(check_point,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Loss tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visdom for local setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visdom\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "vis.close(env=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_tracker(loss_plot, loss_value, num):\n",
    "    '''num, loss_value, are Tensor'''\n",
    "    vis.line(X=num,\n",
    "             Y=loss_value,\n",
    "             win = loss_plot,\n",
    "             update='append'\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plt = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='loss_tracker', legend=['loss'], showlegend=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(logs_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Defining Train, Val, Test Function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, criterion, optimizer, scheduler, val_every, device, saved_dir, writer):\n",
    "    print('Start training..')\n",
    "    torch.cuda.empty_cache()\n",
    "    best_loss = 9999999\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    for epoch in tqdm(range(num_epochs), desc='epoch'):\n",
    "        count = 0.0\n",
    "        # running_loss = 0.0\n",
    "        for i, (imgs, labels) in tqdm(enumerate(data_loader), desc=\"in epoch\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Saving train loss\n",
    "            train_loss_list.append(loss.item())\n",
    "            writer.add_scalar('Loss/Train', loss, i)\n",
    "\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            accuracy = (labels == argmax).float().mean()\n",
    "            # Saving train accuracy\n",
    "            train_accuracy_list.append(accuracy)\n",
    "            writer.add_scalar('Accuracy/Train', accuracy, i)\n",
    "\n",
    "            if (i+1) % 30 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(data_loader), loss.item(), accuracy.item() * 100))\n",
    "            #if i % 30 == 29:\n",
    "                #loss_tracker(loss_plt, torch.Tensor([running_loss/30]), torch.Tensor([i + epoch*len(trainloader) ]))\n",
    "                #running_loss = 0.0\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss, val_accuracy = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            # Saving validation loss and accuracy\n",
    "            val_loss_list.append(avrg_loss)\n",
    "            writer.add_scalar('Loss/Validation', avrg_loss, epoch+1)\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            writer.add_scalar('Accuracy/Validation', val_accuracy, epoch+1)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, epoch, optimizer, avrg_loss, val_accuracy, saved_dir)\n",
    "            else:\n",
    "                count += 1\n",
    "                if count >= 10:\n",
    "                    print('Best performance does not occur within 10 epochs. Early stopping!!')\n",
    "                    scheduler.step()\n",
    "                    writer.flush()\n",
    "                    return train_loss_list, val_loss_list, train_accuracy_list, val_accuracy_list\n",
    "            ''' Only for local setting\n",
    "            loss_tracker(loss_plt, torch.Tensor([avrg_loss]), torch.Tensor([epoch]))'''\n",
    "    scheduler.step()\n",
    "    writer.flush()\n",
    "    return train_loss_list, val_loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch) )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs,labels)\n",
    "            \n",
    "            total += imgs.size(0)\n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "        avrg_loss = total_loss / cnt\n",
    "        val_accuracy = correct / total * 100\n",
    "        print('Validation #{}  Accuracy: {:.2f}%  Average Loss: {:.4f}'.format(epoch, val_accuracy, avrg_loss))\n",
    "    model.train()\n",
    "    return avrg_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    print('Start test..')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (imgs, labels) in tqdm(enumerate(data_loader)):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            _, argmax = torch.max(outputs, 1)\n",
    "            total += imgs.size(0)\n",
    "            correct += (labels == argmax).sum().item()\n",
    "\n",
    "        print('Test accuracy for {} images: {:.2f}%'.format(total, correct / total * 100))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Loss function, Optimizer, Directory for saving]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7777)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(vgg16_transfer.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.00001)\n",
    "\n",
    "val_every = 1\n",
    "saved_dir = './saved/VGG16_Simple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, val_loss_list, train_accuracy_list, val_accuracy_list = train(num_epochs, vgg16_transfer, train_loader, criterion, optimizer, scheduler, val_every, device, saved_dir, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing train loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With matplotlib in colab setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss_list, label = 'Train')\n",
    "plt.plot(val_loss_list, label = 'Validation')\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(train_accuracy_list, label = 'Train')\n",
    "plt.plot(val_accuracy_list, label = 'Validation')\n",
    "# plt.savefig('graph.png',facecolor = 'w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir{logs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_test = VGG(VGG.make_layers(cfg),4,True).to(device)\n",
    "model_path = './saved/VGG16_Simple/.pt'    # .pt 앞에 파일 이름 붙이기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "vgg16_test.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(vgg16_test, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing using matplotlib\n",
    "columns = 5\n",
    "rows = 5\n",
    "fig = plt.figure(figsize=(40,40))\n",
    "\n",
    "vgg16_test.eval()\n",
    "\n",
    "for i in range(1, columns*rows+1):\n",
    "    data_idx = np.random.randint(len(test_data))\n",
    "    input_img = test_data[data_idx][0].unsqueeze(dim=0).to(device) \n",
    "    output = vgg16_test(input_img)\n",
    "    _, argmax = torch.max(output, 1)\n",
    "    pred = argmax.item()\n",
    "    label = test_data[data_idx][1]\n",
    "\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    if pred == 0:\n",
    "        pred_title = 'Normal'\n",
    "    elif pred == 1:\n",
    "        pred_title = 'CNV'\n",
    "    elif pred == 2:\n",
    "        pred_title = 'DME'\n",
    "    elif pred == 3:\n",
    "        pred_title = 'Drusen'\n",
    "    if pred == label:\n",
    "        plt.title(pred_title + '(O)')   # 맞은 경우 (O)로 표시\n",
    "    else:\n",
    "        plt.title(pred_title + '(X)')   # 틀린 경우 (X)로 표시\n",
    "    plot_img = test_data[data_idx][0]\n",
    "    plot_img = plot_img * train_std + train_mean    # 이미지를 normalization 이전 상태로 되돌리는 작업\n",
    "    transforms.functional.to_pil_image(plot_img)\n",
    "    plt.imshow(plot_img)\n",
    "    plt.axis('off')\n",
    "vgg16_test.train()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 임의의 image에 대해 percentage 추출하기\n",
    "\n",
    "idx2label = dict()\n",
    "idx2label[0] = 'Normal'\n",
    "idx2label[1] = 'CNV'\n",
    "idx2label[2] = 'DME'\n",
    "idx2label[3] = 'Drusen'\n",
    "\n",
    "label_list = ['Normal', 'CNV', 'DME', 'Drusen']\n",
    "\n",
    "vgg16_test.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(1,6):\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        data_idx = np.random.randint(len(test_data))\n",
    "\n",
    "        input_img = test_data[data_idx][0].unsqueeze(dim=0).to(device) \n",
    "        output = vgg16_test(input_img)\n",
    "        label = test_data[data_idx][1]\n",
    "        pred_percent = F.softmax(output, dim=1)*100\n",
    "        pred_percent = pred_percent.squeeze().tolist()\n",
    "        pred_dict = {key: value for key, value in zip(label_list, pred_percent)}\n",
    "        pred = sorted(pred_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        plt.figure(figsize=(2,2))\n",
    "        plt.imshow(input_img[0][0], cmap='gray')    # 3channel이라 [0]만 뽑아냄\n",
    "        plt.title(f'Pred: {pred[0][0]}, Label: {idx2label[label]}', fontsize=10)\n",
    "        plt.show()\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            print(f'- {pred[i][0]}: {round(pred[i][1],2)}%')\n",
    "    \n",
    "vgg16_test.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
